{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training_io import load_log, load_vectors\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from scipy.sparse import hstack\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"imdb_trainsize_experiment\"\n",
    "vectors_fn = \"imdb_vectors_full_1e-03_120epoch_p30.jsonl\"\n",
    "log_fn = \"imdb_log_full_1e-03_120epoch_p30.txt\"\n",
    "\n",
    "log = load_log(os.path.join(results_dir, log_fn))\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test = load_vectors(os.path.join(results_dir, vectors_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dir = \"IMDB_splits\"\n",
    "doc_dirs = [\"train.jsonl\", \"dev.jsonl\", \"test.jsonl\"]\n",
    "docs_train, docs_dev, docs_test = [], [], []\n",
    "docs = [docs_train, docs_dev, docs_test]\n",
    "for i, doc_dir in enumerate(doc_dirs):\n",
    "    with open(os.path.join(docs_dir, doc_dirs[i])) as f:\n",
    "        for line in f:\n",
    "            d = json.loads(line)\n",
    "            docs[i].append(d[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_list = [20, 200, 2000, 20000, 60, 30, 10, 20, 200, 2000, 60, 30, 10]\n",
    "repeat_times_list = [10, 10, 10, 1, 10, 10, 10, 20, 20, 20, 20, 20, 20]\n",
    "Cs = np.logspace(-1, 9, 21)\n",
    "multipliers = np.logspace(0, 2, 9) # on DV in the concatenation with BON\n",
    "random_seed = 2\n",
    "result_fn = \"imdb_trainsize_results.json\"\n",
    "sampled_inds_fn = \"imdb_trainsize_sampled_inds.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\n",
    "    \"id\": [], \"train_size\": [], \"C\": [], \"train_acc\": [], \"dev_acc\": [], \"test_acc\": [], \"model\": [], \"multiplier\": []\n",
    "    }\n",
    "sampled_inds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_bon(docs_train, docs_dev, docs_test, y_train, bon_vectorizer):\n",
    "    n = len(docs_train)\n",
    "    if n > 1000:\n",
    "        bon_vectorizer.set_params(min_df=3)\n",
    "    else:\n",
    "        bon_vectorizer.set_params(min_df=2)\n",
    "    bon_train = bon_vectorizer.fit_transform(docs_train)\n",
    "    bon_dev = bon_vectorizer.transform(docs_dev)\n",
    "    bon_test = bon_vectorizer.transform(docs_test)\n",
    "\n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(bon_train, y_train)\n",
    "    prob = nb.feature_log_prob_\n",
    "    r = np.abs(prob[0] - prob[1])\n",
    "    bon_train, bon_dev, bon_test = map(lambda x: x.multiply(r).tocsr(),\n",
    "        [bon_train, bon_dev, bon_test])\n",
    "    return bon_train, bon_dev, bon_test\n",
    "\n",
    "\n",
    "def gridsearch_on_C(model:LogisticRegression, Cs, X_train, X_dev, X_test, y_train, y_dev, y_test):\n",
    "    best_dev_acc = 0.\n",
    "    best_C = None\n",
    "    test_acc = 0.\n",
    "    for C in Cs:\n",
    "        model.set_params(C=C)\n",
    "        model.fit(X_train, y_train)\n",
    "        dev_acc = model.score(X_dev, y_dev)\n",
    "        if dev_acc > best_dev_acc:\n",
    "            best_dev_acc = dev_acc\n",
    "            best_C = C\n",
    "    model.set_params(C=best_C)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    return best_C, train_acc, dev_acc, test_acc\n",
    "\n",
    "def normalize_text(text):\n",
    "    '''\n",
    "    preprocess a doc from the original imdb dataset\n",
    "    '''\n",
    "    text = re.sub(r'([\\.\",\\(\\)\\!\\?:;])', r' \\1 ', text.lower())  # find listed punctuation marks and add a space in each side\n",
    "    text = re.sub('<br />|\\x85', ' ', text)  # replace non-informational tag/symbol with space (remove them)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181/181 [2:38:58<00:00, 52.70s/it]  \n"
     ]
    }
   ],
   "source": [
    "os.chdir(results_dir)\n",
    "bon_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 3), preprocessor=normalize_text)\n",
    "rs = np.random.RandomState(random_seed)\n",
    "test_id = 0\n",
    "pbar = tqdm(total = sum(repeat_times_list))\n",
    "for train_size, repeat_times in zip(train_size_list, repeat_times_list):\n",
    "    for i in range(repeat_times):\n",
    "        # sample the training set\n",
    "        if train_size < 20000:\n",
    "            train_inds, _ = train_test_split(np.arange(len(X_train)), train_size=train_size, random_state=rs, stratify=y_train)\n",
    "        else:\n",
    "            train_inds = rs.permutation(20000)\n",
    "        X_train_sampled = X_train[train_inds]\n",
    "        y_train_sampled = y_train[train_inds]\n",
    "        docs_train_sampled = [docs_train[d_i] for d_i in train_inds]\n",
    "        sampled_inds[test_id] = train_inds.tolist() # save the sampled train inds\n",
    "        # get bon \n",
    "        bon_train, bon_dev, bon_test = get_nb_bon(docs_train_sampled, docs_dev, docs_test, y_train_sampled, bon_vectorizer)\n",
    "        for model_, multipliers_ in zip([\"DV\", \"BON\", \"DV + BON\"], [[1.], [1.], multipliers]):\n",
    "            for multiplier in multipliers_:\n",
    "                if model_==\"DV\":\n",
    "                    X_train_m, X_dev_m, X_test_m = X_train_sampled, X_dev, X_test\n",
    "                elif model_==\"BON\":\n",
    "                    X_train_m, X_dev_m, X_test_m = bon_train, bon_dev, bon_test\n",
    "                elif model_==\"DV + BON\":\n",
    "                    X_train_m, X_dev_m, X_test_m = hstack((bon_train, X_train_sampled*multiplier), \"csr\"),\\\n",
    "                        hstack((bon_dev, X_dev*multiplier), \"csr\"),\\\n",
    "                        hstack((bon_test, X_test*multiplier), \"csr\")\n",
    "                # tune and train the model\n",
    "                model = LogisticRegression()\n",
    "                best_C, train_acc, dev_acc, test_acc = gridsearch_on_C(model, \n",
    "                    Cs, X_train_m, X_dev_m, X_test_m, y_train_sampled, y_dev, y_test)\n",
    "\n",
    "                # save the result and sampled ids\n",
    "                res['id'].append(test_id)\n",
    "                res['train_size'].append(train_size)\n",
    "                res[\"C\"].append(best_C)\n",
    "                res[\"multiplier\"].append(multiplier)\n",
    "                res[\"train_acc\"].append(train_acc)\n",
    "                res[\"dev_acc\"].append(dev_acc)\n",
    "                res[\"test_acc\"].append(test_acc)\n",
    "                res[\"model\"].append(model_)\n",
    "\n",
    "        test_id += 1\n",
    "        pbar.update()\n",
    "\n",
    "    with open(result_fn, 'w') as f:\n",
    "        json.dump(res, f)\n",
    "    with open(sampled_inds_fn, 'w') as f:\n",
    "        json.dump(sampled_inds, f)\n",
    "pbar.close()\n",
    "os.chdir('..')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9fc8ad3fcac5993ab5f996cc3f7cdfaaef9e49e69a35393c969fa019ab25aee"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
